# DQN –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∑–∞–¥–∞–Ω–∏–π

Deep Q-Network (DQN) –º–æ–¥–µ–ª—å –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Reinforcement Learning –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞–º.

## –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

### ‚úÖ –ß—Ç–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –º–æ–¥–µ–ª—å DQN:

1. **üìä BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** - –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (know, learn, guess, slip) –¥–ª—è –≤—Å–µ—Ö –Ω–∞–≤—ã–∫–æ–≤ —Å—Ç—É–¥–µ–Ω—Ç–∞
2. **üìà –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ø—ã—Ç–æ–∫** - –£—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ø—ã—Ç–æ–∫ —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
3. **üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞–Ω–∏–π** - –°–ª–æ–∂–Ω–æ—Å—Ç—å, —Ç–∏–ø –∑–∞–¥–∞–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏
4. **üîó –ì—Ä–∞—Ñ –Ω–∞–≤—ã–∫–æ–≤** - –°—Ç—Ä–æ–≥–æ–µ —Å–æ–±–ª—é–¥–µ–Ω–∏–µ prerequisite –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π

### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞ = BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã + –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ø—ã—Ç–æ–∫
                     ‚Üì
            StudentStateEncoder (LSTM + FC)
                     ‚Üì
                Q-Network (FC —Å–ª–æ–∏)
                     ‚Üì
          Q-values –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è
                     ‚Üì
    –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å —É—á—ë—Ç–æ–º available_actions
```

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
mlmodels/dqn/
‚îú‚îÄ‚îÄ __init__.py              # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è
‚îú‚îÄ‚îÄ model.py                 # DQN –º–æ–¥–µ–ª—å, –∞–≥–µ–Ω—Ç, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ data_processor.py        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Django –º–æ–¥–µ–ª–µ–π
‚îú‚îÄ‚îÄ environment.py           # –°—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞
‚îú‚îÄ‚îÄ trainer.py               # –¢—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DQN
‚îú‚îÄ‚îÄ train_dqn.py            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
‚îú‚îÄ‚îÄ test_dqn.py             # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
‚îî‚îÄ‚îÄ README.md               # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

### 1. DQNConfig
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤
- RL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (gamma, epsilon, learning_rate)
- –í–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –Ω–∞–≥—Ä–∞–¥—ã

### 2. DQNAgent
–û—Å–Ω–æ–≤–Ω–æ–π RL –∞–≥–µ–Ω—Ç —Å:
- Q-network –∏ target network
- Experience replay buffer
- Epsilon-greedy exploration
- Double DQN –æ–±—É—á–µ–Ω–∏–µ

### 3. DQNDataProcessor
–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π:
- –ò–∑–≤–ª–µ–∫–∞–µ—Ç BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –±–∞–∑—ã
- –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ø—ã—Ç–æ–∫ —Å—Ç—É–¥–µ–Ω—Ç–∞
- –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è —Å —É—á—ë—Ç–æ–º prerequisite
- –ö–æ–¥–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è DQN

### 4. DQNEnvironment
–°—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è:
- –ú–æ–¥–µ–ª–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å—Ç—É–¥–µ–Ω—Ç-–∑–∞–¥–∞–Ω–∏–µ
- –í—ã—á–∏—Å–ª—è–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —ç–ø–∏–∑–æ–¥—ã

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
python -m mlmodels.dqn.train_dqn --test_only

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 500 —ç–ø–∏–∑–æ–¥–∞—Ö
python -m mlmodels.dqn.train_dqn --episodes 500

# –û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
python -m mlmodels.dqn.train_dqn --evaluate_only checkpoints/best_model.pth
```

### –ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from mlmodels.dqn.model import DQNConfig, create_dqn_agent
from mlmodels.dqn.data_processor import DQNDataProcessor
from mlmodels.dqn.trainer import DQNTrainer

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ
config = DQNConfig()
trainer = DQNTrainer(config)
trainer.train(num_episodes=1000)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
processor = DQNDataProcessor()
state_data = processor.get_student_state(student_id=123)

# –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ –∑–∞–¥–∞–Ω–∏–µ
agent = trainer.agent
recommended_task = agent.select_action(
    state=state_data['encoded_state'],
    available_actions=state_data['available_actions']
)
```

## –£—á—ë—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π

### –ì—Ä–∞—Ñ –Ω–∞–≤—ã–∫–æ–≤ –∏ prerequisite

–ú–æ–¥–µ–ª—å —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –Ω–∞–≤—ã–∫–æ–≤:

```python
# –ü—Ä–∏–º–µ—Ä: –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è "–ò–Ω—Ç–µ–≥—Ä–∞–ª—ã" –Ω—É–∂–Ω–æ –æ—Å–≤–æ–∏—Ç—å "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ"
skills_graph = {
    'derivatives': set(),  # –ë–∞–∑–æ–≤—ã–π –Ω–∞–≤—ã–∫
    'integrals': {'derivatives'},  # –¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ
    'differential_equations': {'derivatives', 'integrals'}  # –¢—Ä–µ–±—É–µ—Ç –æ–±–∞
}

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
available_tasks = env.get_available_actions(mastered_skills)
```

### –°–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥

–ù–∞–≥—Ä–∞–¥–∞ —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤:

```python
reward = base_reward(success) + 
         skill_improvement_bonus(bkt_changes) +
         difficulty_match_bonus(optimal_difficulty) -
         prerequisite_penalty(violations)
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è:
- üìà –ì—Ä–∞—Ñ–∏–∫–∏ loss, rewards, success rate
- üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ N —ç–ø–∏–∑–æ–¥–æ–≤
- üìä JSON —Ñ–∞–π–ª—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
- üéØ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥–µ

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
python -m mlmodels.dqn.test_dqn

# –¢–µ—Å—Ç—ã –ø—Ä–æ–≤–µ—Ä—è—é—Ç:
# - –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
# - –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–∞
# - –°–æ–±–ª—é–¥–µ–Ω–∏–µ prerequisite –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
# - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥
# - –°–æ–∑–¥–∞–Ω–∏–µ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
# - –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
```

## –û—Ç–ª–∏—á–∏—è –æ—Ç DKN

| –ê—Å–ø–µ–∫—Ç | DKN (Supervised) | DQN (Reinforcement Learning) |
|--------|------------------|------------------------------|
| **–ü–æ–¥—Ö–æ–¥** | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Å–ø–µ—Ö–∞ | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è |
| **–¶–µ–ª—å** | –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è | –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥ |
| **–û–±—É—á–µ–Ω–∏–µ** | –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã | –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã |
| **–ê–¥–∞–ø—Ç–∞—Ü–∏—è** | –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö | –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ |
| **Explore/Exploit** | –ù–µ—Ç | Epsilon-greedy –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ |

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º

–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:
- 5+ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –ø–æ–ø—ã—Ç–∫–∞–º–∏
- 3+ –Ω–∞–≤—ã–∫–∞ —Å prerequisite —Å–≤—è–∑—è–º–∏  
- 10+ –∑–∞–¥–∞–Ω–∏–π —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
- BKT –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ `DQNConfig`:

```python
config = DQNConfig()
config.num_actions = 270        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞–Ω–∏–π
config.gamma = 0.99            # –î–∏—Å–∫–æ–Ω—Ç —Ñ–∞–∫—Ç–æ—Ä
config.epsilon_start = 1.0     # –ù–∞—á–∞–ª—å–Ω–∞—è exploration
config.learning_rate = 0.001   # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
config.batch_size = 32         # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
config.memory_size = 10000     # –†–∞–∑–º–µ—Ä replay buffer
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
- **Episode Reward** - –æ–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥
- **Success Rate** - –¥–æ–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
- **Epsilon** - —Ç–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å exploration
- **Loss** - –æ—à–∏–±–∫–∞ Q-network
- **Episode Length** - –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Django

–ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å Django –º–æ–¥–µ–ª—è–º–∏:
- `User` –∏ `StudentProfile` - —Å—Ç—É–¥–µ–Ω—Ç—ã
- `Skill` - –Ω–∞–≤—ã–∫–∏ —Å prerequisite
- `Task` - –∑–∞–¥–∞–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- `TaskAttempt` - –ø–æ–ø—ã—Ç–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
- `StudentSkillMastery` - –ø—Ä–æ–≥—Ä–µ—Å—Å BKT

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤:

### –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```python
config.num_episodes = 100
config.batch_size = 16
config.memory_size = 1000
```

### –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
```python
config.num_episodes = 2000
config.batch_size = 32
config.memory_size = 10000
```

### –ü—Ä–æ–¥–∞–∫—à–Ω
```python
config.num_episodes = 5000+
config.batch_size = 64
config.memory_size = 50000
```
