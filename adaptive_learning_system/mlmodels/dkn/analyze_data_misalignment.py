#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –º–µ–∂–¥—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è DKN

–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""

import os
import sys
import django
from pathlib import Path
import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Any
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Django
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'adaptive_learning.settings')
django.setup()

from django.contrib.auth.models import User
from skills.models import Skill
from methodist.models import Task
from mlmodels.models import TaskAttempt, StudentSkillMastery
from student.models import StudentProfile
from data_processor import DKNDataProcessor


def analyze_data_misalignment():
    """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –º–µ–∂–¥—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    print("üîç –ê–ù–ê–õ–ò–ó –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–ô –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    synthetic_path = Path("dataset/enhanced_synthetic_dataset.csv")
    if not synthetic_path.exists():
        print("‚ùå –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return
    
    df_synthetic = pd.read_csv(synthetic_path)
    print(f"üìä –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π: {len(df_synthetic)}")
    
    # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    real_students = User.objects.filter(student_profile__isnull=False)
    real_attempts = TaskAttempt.objects.all()
    real_masteries = StudentSkillMastery.objects.all()
    
    print(f"üìä –†–µ–∞–ª—å–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤: {real_students.count()}")
    print(f"üìä –†–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫: {real_attempts.count()}")
    print(f"üìä –†–µ–∞–ª—å–Ω—ã—Ö –æ—Å–≤–æ–µ–Ω–∏–π –Ω–∞–≤—ã–∫–æ–≤: {real_masteries.count()}")
    
    return df_synthetic, real_students, real_attempts, real_masteries


def identify_feature_mismatches(df_synthetic):
    """–í—ã—è–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"""
    print(f"\nüéØ –í–´–Ø–í–õ–ï–ù–ò–ï –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–ô –ü–†–ò–ó–ù–ê–ö–û–í")
    print("=" * 60)
    
    print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø:")
    print("-" * 40)
    
    # 1. BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print("1. üß† BKT –ü–ê–†–ê–ú–ï–¢–†–´:")
    
    # –†–µ–∞–ª—å–Ω—ã–µ BKT –∏–∑ –±–∞–∑—ã
    real_bkt_stats = {}
    masteries = StudentSkillMastery.objects.all()
    if masteries.exists():
        p_l_values = [m.current_mastery_prob for m in masteries]
        p_t_values = [m.transition_prob for m in masteries]
        p_g_values = [m.guess_prob for m in masteries]
        p_s_values = [m.slip_prob for m in masteries]
        
        real_bkt_stats = {
            'P_L': {'mean': np.mean(p_l_values), 'std': np.std(p_l_values), 'min': np.min(p_l_values), 'max': np.max(p_l_values)},
            'P_T': {'mean': np.mean(p_t_values), 'std': np.std(p_t_values), 'min': np.min(p_t_values), 'max': np.max(p_t_values)},
            'P_G': {'mean': np.mean(p_g_values), 'std': np.std(p_g_values), 'min': np.min(p_g_values), 'max': np.max(p_g_values)},
            'P_S': {'mean': np.mean(p_s_values), 'std': np.std(p_s_values), 'min': np.min(p_s_values), 'max': np.max(p_s_values)}
        }
        
        print("   üìä –†–µ–∞–ª—å–Ω—ã–µ BKT —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:")
        for param, stats in real_bkt_stats.items():
            print(f"     {param}: {stats['mean']:.3f} ¬± {stats['std']:.3f} (–¥–∏–∞–ø–∞–∑–æ–Ω: {stats['min']:.3f}-{stats['max']:.3f})")
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ BKT
    print("   üìä –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ BKT:")
    skill_learned_cols = [col for col in df_synthetic.columns if 'skill_' in col and '_learned' in col]
    skill_transit_cols = [col for col in df_synthetic.columns if 'skill_' in col and '_transit' in col]
    
    if skill_learned_cols:
        synthetic_p_l = df_synthetic[skill_learned_cols[0]].values
        print(f"     P_L: {np.mean(synthetic_p_l):.3f} ¬± {np.std(synthetic_p_l):.3f}")
    
    if skill_transit_cols:
        synthetic_p_t = df_synthetic[skill_transit_cols[0]].values
        print(f"     P_T: {np.mean(synthetic_p_t):.3f} ¬± {np.std(synthetic_p_t):.3f}")
    
    # 2. –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ø—ã—Ç–æ–∫
    print(f"\n2. üìö –ò–°–¢–û–†–ò–Ø –ü–û–ü–´–¢–û–ö:")
    
    # –†–µ–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
    processor = DKNDataProcessor(max_history_length=10)
    student_with_data = User.objects.get(id=7)  # –°—Ç—É–¥–µ–Ω—Ç —Å –¥–∞–Ω–Ω—ã–º–∏
    student_profile = StudentProfile.objects.get(user=student_with_data)
    
    real_attempts_user = TaskAttempt.objects.filter(student=student_profile)
    if real_attempts_user.exists():
        real_success_rate = real_attempts_user.filter(is_correct=True).count() / real_attempts_user.count()
        real_times = [a.time_spent for a in real_attempts_user if a.time_spent]
        real_avg_time = np.mean(real_times) if real_times else 0
        
        print(f"   üìä –†–µ–∞–ª—å–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏:")
        print(f"     –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {real_success_rate:.1%}")
        print(f"     –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {real_avg_time:.1f}—Å")
        print(f"     –í—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {real_attempts_user.count()}")
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è
    synthetic_success = df_synthetic['target'].mean()
    hist_time_cols = [col for col in df_synthetic.columns if 'hist_' in col and '_time' in col]
    if hist_time_cols:
        synthetic_times = df_synthetic[hist_time_cols].values.flatten()
        synthetic_times = synthetic_times[synthetic_times > 0]  # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏
        synthetic_avg_time = np.mean(synthetic_times) if len(synthetic_times) > 0 else 0
        
        print(f"   üìä –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ø—ã—Ç–∫–∏:")
        print(f"     –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {synthetic_success:.1%}")
        print(f"     –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {synthetic_avg_time:.1f}—Å")
    
    # 3. –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ —Ç–∏–ø—ã –∑–∞–¥–∞–Ω–∏–π
    print(f"\n3. üìù –ó–ê–î–ê–ù–ò–Ø:")
    
    # –†–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è
    real_tasks = Task.objects.all()
    real_difficulties = [task.difficulty for task in real_tasks]
    real_types = [task.task_type for task in real_tasks]
    
    difficulty_counts = {}
    type_counts = {}
    
    for diff in real_difficulties:
        difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1
    
    for t_type in real_types:
        type_counts[t_type] = type_counts.get(t_type, 0) + 1
    
    print(f"   üìä –†–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è:")
    print(f"     –°–ª–æ–∂–Ω–æ—Å—Ç–∏: {difficulty_counts}")
    print(f"     –¢–∏–ø—ã: {type_counts}")
      # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è
    synthetic_difficulties = df_synthetic['task_difficulty'].value_counts().to_dict()
    synthetic_types = df_synthetic['task_type'].value_counts().to_dict()
    
    print(f"   üìä –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è:")
    print(f"     –°–ª–æ–∂–Ω–æ—Å—Ç–∏: {synthetic_difficulties}")
    print(f"     –¢–∏–ø—ã: {synthetic_types}")
    
    return real_bkt_stats, {
        'real_success_rate': real_success_rate if real_attempts_user.exists() else 0,
        'synthetic_success_rate': synthetic_success,
        'real_difficulty_dist': difficulty_counts,
        'synthetic_difficulty_dist': synthetic_difficulties,
        'real_type_dist': type_counts,
        'synthetic_type_dist': synthetic_types
    }


def calculate_distribution_divergence(real_bkt_stats, comparison_stats):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏"""
    print(f"\nüìä –†–ê–°–ß–ï–¢ –†–ê–°–•–û–ñ–î–ï–ù–ò–ô –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô")
    print("=" * 60)
    
    divergences = {}
    
    # 1. –£—Å–ø–µ—à–Ω–æ—Å—Ç—å
    success_diff = abs(comparison_stats['real_success_rate'] - comparison_stats['synthetic_success_rate'])
    divergences['success_rate'] = success_diff
    print(f"üéØ –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏: {success_diff:.1%}")
    
    # 2. –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏–π
    real_diff_dist = comparison_stats['real_difficulty_dist']
    synthetic_diff_dist = comparison_stats['synthetic_difficulty_dist']
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    real_total = sum(real_diff_dist.values())
    synthetic_total = sum(synthetic_diff_dist.values())
    
    difficulty_divergence = 0
    all_difficulties = set(list(real_diff_dist.keys()) + list(synthetic_diff_dist.keys()))
    
    for diff in all_difficulties:
        real_prob = real_diff_dist.get(diff, 0) / real_total
        synthetic_prob = synthetic_diff_dist.get(diff, 0) / synthetic_total
        difficulty_divergence += abs(real_prob - synthetic_prob)
    
    divergences['difficulty'] = difficulty_divergence
    print(f"üìù –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {difficulty_divergence:.3f}")
      # 3. –¢–∏–ø—ã –∑–∞–¥–∞–Ω–∏–π
    real_type_dist = comparison_stats['real_type_dist']
    synthetic_type_dist = comparison_stats['synthetic_type_dist']
    
    real_type_total = sum(real_type_dist.values())
    synthetic_type_total = sum(synthetic_type_dist.values())
    
    type_divergence = 0
    all_types = set(list(real_type_dist.keys()) + list(synthetic_type_dist.keys()))
    
    for t_type in all_types:
        real_prob = real_type_dist.get(t_type, 0) / real_type_total
        synthetic_prob = synthetic_type_dist.get(t_type, 0) / synthetic_type_total
        type_divergence += abs(real_prob - synthetic_prob)
    
    divergences['task_type'] = type_divergence
    print(f"üìã –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤: {type_divergence:.3f}")
    
    return divergences


def identify_redundant_features(df_synthetic):
    """–í—ã—è–≤–ª—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ/–Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    print(f"\nüîç –í–´–Ø–í–õ–ï–ù–ò–ï –ò–ó–ë–´–¢–û–ß–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print("=" * 60)
    
    # 1. –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    numeric_cols = df_synthetic.select_dtypes(include=[np.number]).columns
    correlation_matrix = df_synthetic[numeric_cols].corr()
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = abs(correlation_matrix.iloc[i, j])
            if corr_val > 0.95:  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                high_corr_pairs.append((
                    correlation_matrix.columns[i],
                    correlation_matrix.columns[j],
                    corr_val
                ))
    
    print(f"üîÑ –í—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (r > 0.95):")
    for col1, col2, corr in high_corr_pairs:
        print(f"   {col1} ‚Üî {col2}: {corr:.3f}")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    target_correlations = correlation_matrix['target'].abs().sort_values(ascending=False)
    
    print(f"\nüìà –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (—Ç–æ–ø-10):")
    for feature, corr in target_correlations.head(10).items():
        if feature != 'target':
            print(f"   {feature}: {corr:.3f}")
    
    print(f"\nüìâ –°–ª–∞–±–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å —Ü–µ–ª—å—é (< 0.1):")
    weak_features = target_correlations[target_correlations < 0.1]
    for feature, corr in weak_features.items():
        if feature != 'target':
            print(f"   {feature}: {corr:.3f}")
    
    # 3. –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    constant_features = []
    for col in numeric_cols:
        if df_synthetic[col].nunique() == 1:
            constant_features.append(col)
    
    if constant_features:
        print(f"\nüö´ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        for feature in constant_features:
            print(f"   {feature}: {df_synthetic[feature].iloc[0]}")
    
    return high_corr_pairs, weak_features.index.tolist(), constant_features


def suggest_data_optimization(divergences, redundant_features):
    """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\nüí° –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    print("üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
    print("-" * 40)
    
    critical_issues = []
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
    if divergences.get('success_rate', 0) > 0.2:  # –ë–æ–ª–µ–µ 20% —Ä–∞–∑–Ω–∏—Ü–∞
        critical_issues.append("–ë–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏")
        print("‚ùå –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ä–µ–∞–ª—å–Ω–æ–π")
    
    if divergences.get('difficulty', 0) > 0.5:
        critical_issues.append("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
        print("‚ùå –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏–π –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–º—É")
    
    if divergences.get('task_type', 0) > 0.5:
        critical_issues.append("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞–Ω–∏–π")
        print("‚ùå –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞–Ω–∏–π –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–º—É")
    
    # 2. –ò–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    high_corr_pairs, weak_features, constant_features = redundant_features
    
    if high_corr_pairs:
        critical_issues.append("–ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å")
        print(f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(high_corr_pairs)} –ø–∞—Ä –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    if len(weak_features) > 5:
        critical_issues.append("–ú–Ω–æ–≥–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"‚ùå {len(weak_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–ª–∞–±–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å —Ü–µ–ª—å—é")
    
    if constant_features:
        critical_issues.append("–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
        print(f"‚ùå {len(constant_features)} –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    print(f"\n‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
    print("-" * 40)
    
    recommendations = []
    
    # 1. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    if divergences.get('success_rate', 0) > 0.1:
        recommendations.append("–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é")
        print("1. üéØ –ü–µ—Ä–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é:")
        print("   - –ò–∑–º–µ–Ω–∏—Ç—å –∞—Ä—Ö–µ—Ç–∏–ø—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏")
        print("   - –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫")
    
    if divergences.get('difficulty', 0) > 0.3:
        recommendations.append("–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
        print("2. üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏–π:")
        print("   - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –∑–∞–¥–∞–Ω–∏–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π")
        print("   - –£–±—Ä–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if high_corr_pairs:
        recommendations.append("–£—Å—Ç—Ä–∞–Ω–∏—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å")
        print("3. üîÑ –£—Å—Ç—Ä–∞–Ω–∏—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å:")
        for col1, col2, corr in high_corr_pairs[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            print(f"   - –£–¥–∞–ª–∏—Ç—å –æ–¥–∏–Ω –∏–∑: {col1} –∏–ª–∏ {col2}")
    
    if len(weak_features) > 5:
        recommendations.append("–£–¥–∞–ª–∏—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
        print("4. üóëÔ∏è –£–¥–∞–ª–∏—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        print(f"   - –£–¥–∞–ª–∏—Ç—å {len(weak_features)} —Å–ª–∞–±–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   - –ù–∞–ø—Ä–∏–º–µ—Ä: {', '.join(weak_features[:5])}")
    
    if constant_features:
        recommendations.append("–£–¥–∞–ª–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã")
        print("5. üö´ –£–¥–∞–ª–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        for feature in constant_features[:3]:
            print(f"   - –£–¥–∞–ª–∏—Ç—å: {feature}")
    
    # 3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüöÄ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:")
    print("-" * 40)
    print("6. üìä –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏:")
    print("   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ")
    print("   - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    print("   - –î–æ–±–∞–≤–∏—Ç—å —à—É–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏")
    
    print("7. üéØ –§–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö:")
    print("   - –£–≤–µ–ª–∏—á–∏—Ç—å –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∞–∂–Ω—ã—Ö BKT –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print("   - –£–ª—É—á—à–∏—Ç—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ø—ã—Ç–æ–∫")
    print("   - –î–æ–±–∞–≤–∏—Ç—å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
    
    return {
        'critical_issues': critical_issues,
        'recommendations': recommendations,
        'features_to_remove': list(set(weak_features + constant_features + [pair[1] for pair in high_corr_pairs]))
    }


def generate_optimization_script(optimization_plan):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    script_content = '''#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DKN

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
"""

import pandas as pd
import numpy as np
from pathlib import Path

def optimize_synthetic_data():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ"""
      # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv("dataset/enhanced_synthetic_dataset.csv")
    print(f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {{len(df)}}")
    
    # –£–¥–∞–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features_to_remove = {features_to_remove}
    df_optimized = df.drop(columns=[col for col in features_to_remove if col in df.columns])
    print(f"–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {{len([col for col in features_to_remove if col in df.columns])}}")
    
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    current_success_rate = df_optimized['target'].mean()
    target_success_rate = 0.75  # –¶–µ–ª–µ–≤–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    if abs(current_success_rate - target_success_rate) > 0.1:
        print(f"–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {{current_success_rate:.1%}} -> {{target_success_rate:.1%}}")
        
        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        success_samples = df_optimized[df_optimized['target'] == 1]
        failure_samples = df_optimized[df_optimized['target'] == 0]
        
        target_success_count = int(len(df_optimized) * target_success_rate)
        target_failure_count = len(df_optimized) - target_success_count
        
        # –ü–µ—Ä–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º
        if len(success_samples) > target_success_count:
            success_samples = success_samples.sample(n=target_success_count, random_state=42)
        if len(failure_samples) > target_failure_count:
            failure_samples = failure_samples.sample(n=target_failure_count, random_state=42)
        
        df_optimized = pd.concat([success_samples, failure_samples]).sample(frac=1, random_state=42)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    output_path = "dataset/optimized_synthetic_dataset.csv"
    df_optimized.to_csv(output_path, index=False)
    print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {{output_path}}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {{len(df_optimized)}}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {{len(df_optimized.columns)}}")
    
    return df_optimized

if __name__ == "__main__":
    optimize_synthetic_data()
'''.format(features_to_remove=optimization_plan['features_to_remove'])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–ø—Ç
    script_path = Path("optimize_synthetic_data.py")
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"\nüìÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Å–∫—Ä–∏–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {script_path}")
    return script_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π"""
    print("üîç –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–ô –î–ê–ù–ù–´–• DKN")
    print("=" * 70)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df_synthetic, real_students, real_attempts, real_masteries = analyze_data_misalignment()
        
        # 2. –í—ã—è–≤–ª—è–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        real_bkt_stats, comparison_stats = identify_feature_mismatches(df_synthetic)
          # 3. –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
        divergences = calculate_distribution_divergence(real_bkt_stats, comparison_stats)
        
        # 4. –ù–∞—Ö–æ–¥–∏–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        redundant_features = identify_redundant_features(df_synthetic)
        
        # 5. –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        optimization_plan = suggest_data_optimization(divergences, redundant_features)
        
        # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        script_path = generate_optimization_script(optimization_plan)
        
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –ó–∞–ø—É—Å—Ç–∏—Ç–µ {script_path} –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.")
        
    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
