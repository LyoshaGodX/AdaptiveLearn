#!/usr/bin/env python3
"""
Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ DKN - ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ
"""

import os
import sys
import django
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging
import time
import json

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Django
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'adaptive_learning.settings')
django.setup()

# Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def quick_train():
    """Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
    logger.info("=== Ð‘Ð«Ð¡Ð¢Ð ÐžÐ• ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• DKN ===")
    
    try:
        # 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 10K Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð² Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸)
        logger.info("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        df = pd.read_csv('mlmodels/dkn/dataset/enhanced_synthetic_dataset.csv')
        df_small = df.sample(n=10000, random_state=42).reset_index(drop=True)
        
        # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ðµ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ
        train_size = int(0.8 * len(df_small))
        train_df = df_small[:train_size]
        val_df = df_small[train_size:]
        
        logger.info(f"Ð”Ð°Ð½Ð½Ñ‹Ðµ: {len(train_df)} train, {len(val_df)} val")
        
        # 2. ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
        def simple_prepare(data_df):
            X, y = [], []
            for _, row in data_df.iterrows():
                # ÐŸÑ€Ð¾ÑÑ‚Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸: BKT Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ + Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ
                features = [
                    row.get('skill_0_learned', 0.5),
                    row.get('skill_0_transit', 0.1),
                    float(row.get('task_difficulty', 0)) / 2.0,  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
                    float(row.get('task_type', 0)) / 2.0,
                    float(row['task_id'] % 10) / 10.0,  # ID Ð·Ð°Ð´Ð°Ð½Ð¸Ñ (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð¾)
                ]
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ (ÑÑ€ÐµÐ´Ð½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ)
                hist_features = []
                for i in range(5):  # Ð¢Ð¾Ð»ÑŒÐºÐ¾ 5 Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ…
                    hist_features.extend([
                        row.get(f'hist_{i}_correct', 0.0),
                        row.get(f'hist_{i}_score', 0.0),
                        row.get(f'hist_{i}_time', 60.0) / 100.0  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
                    ])
                
                features.extend(hist_features)
                X.append(features)
                y.append(float(row['target']))
            
            return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
        
        X_train, y_train = simple_prepare(train_df)
        X_val, y_val = simple_prepare(val_df)
        
        logger.info(f"Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²: {X_train.shape[1]}")
        
        # 3. ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        class SimpleDKN(nn.Module):
            def __init__(self, input_size):
                super().__init__()
                self.model = nn.Sequential(
                    nn.Linear(input_size, 64),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(64, 32),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(32, 16),
                    nn.ReLU(),
                    nn.Linear(16, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                return self.model(x).squeeze()
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = SimpleDKN(X_train.shape[1]).to(device)
        
        logger.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð½Ð° {device}")
        logger.info(f"ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²: {sum(p.numel() for p in model.parameters()):,}")
        
        # 4. ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¸ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¹
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)
        criterion = nn.BCELoss()
        
        # 5. Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
        logger.info("ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ...")
        
        X_train, y_train = X_train.to(device), y_train.to(device)
        X_val, y_val = X_val.to(device), y_val.to(device)
        
        best_val_acc = 0.0
        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
        
        for epoch in range(20):
            start_time = time.time()
            
            # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            model.train()
            optimizer.zero_grad()
            
            train_pred = model(X_train)
            train_loss = criterion(train_pred, y_train)
            train_loss.backward()
            optimizer.step()
            
            # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
            model.eval()
            with torch.no_grad():
                val_pred = model(X_val)
                val_loss = criterion(val_pred, y_val)
                
                # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
                train_acc = ((train_pred > 0.5) == y_train).float().mean().item()
                val_acc = ((val_pred > 0.5) == y_val).float().mean().item()
            
            epoch_time = time.time() - start_time
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
            history['train_loss'].append(train_loss.item())
            history['val_loss'].append(val_loss.item())
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
            logger.info(f"Ð­Ð¿Ð¾Ñ…Ð° {epoch+1:2d} ({epoch_time:.1f}s): "
                       f"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
                       f"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'best_simple_model.pth')
                logger.info(f"  >>> ÐÐ¾Ð²Ð°Ñ Ð»ÑƒÑ‡ÑˆÐ°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_val_acc:.4f}")
            
            # Early stopping
            if epoch > 5 and val_acc < max(history['val_acc'][-5:]) - 0.01:
                logger.info(f"Early stopping Ð½Ð° ÑÐ¿Ð¾Ñ…Ðµ {epoch+1}")
                break
        
        # 6. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        logger.info(f"ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾! Ð›ÑƒÑ‡ÑˆÐ°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_val_acc:.4f}")
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        results = {
            'best_validation_accuracy': best_val_acc,
            'final_train_accuracy': history['train_acc'][-1],
            'final_val_accuracy': history['val_acc'][-1],
            'num_epochs': len(history['train_acc']),
            'history': history,
            'model_parameters': sum(p.numel() for p in model.parameters()),
            'data_size': len(train_df)
        }
        
        with open('quick_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info("Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² quick_results.json")
        
        return model, results
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None

if __name__ == "__main__":
    model, results = quick_train()
    if results:
        print(f"\nðŸŽ¯ Ð˜Ð¢ÐžÐ“ÐžÐ’Ð«Ð• Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð«:")
        print(f"ðŸ“Š Ð›ÑƒÑ‡ÑˆÐ°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {results['best_validation_accuracy']:.4f}")
        print(f"ðŸ”¢ ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸: {results['model_parameters']:,}")
        print(f"ðŸ“ˆ Ð­Ð¿Ð¾Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {results['num_epochs']}")
        print(f"ðŸ“ Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {results['data_size']:,} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²")
