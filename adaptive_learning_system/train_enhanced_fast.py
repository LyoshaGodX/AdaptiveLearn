#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å DKN —Å –ª—É—á—à–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
"""

import os
import sys
import django
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging
import time
import json
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Django
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'adaptive_learning.settings')
django.setup()

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_enhanced_features(df):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏"""
    features_list = []
    targets = []
    
    for _, row in df.iterrows():
        features = []
        
        # 1. BKT –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ—Å–Ω–æ–≤–Ω—ã–µ)
        skill_learned = row.get('skill_0_learned', 0.5)
        skill_transit = row.get('skill_0_transit', 0.1)
        features.extend([skill_learned, skill_transit])
        
        # 2. –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∑–∞–¥–∞–Ω–∏—è
        task_difficulty = float(row.get('task_difficulty', 0))
        task_type = float(row.get('task_type', 0))
        features.extend([task_difficulty, task_type])
        
        # 3. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        hist_correct_sum = 0
        hist_score_avg = 0
        hist_time_avg = 0
        hist_count = 0
        
        for i in range(10):
            hist_correct = row.get(f'hist_{i}_correct', 0)
            hist_score = row.get(f'hist_{i}_score', 0.0)
            hist_time = row.get(f'hist_{i}_time', 60.0)
            
            if hist_correct > 0 or hist_score > 0:
                hist_correct_sum += hist_correct
                hist_score_avg += hist_score
                hist_time_avg += hist_time
                hist_count += 1
        
        if hist_count > 0:
            hist_score_avg /= hist_count
            hist_time_avg /= hist_count
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.extend([
            hist_correct_sum,  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            hist_score_avg,    # –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä
            hist_time_avg / 100.0,  # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            hist_count,        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        ])
        
        # 4. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # –¢—Ä–µ–Ω–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ vs –ø–µ—Ä–≤—ã–µ –ø–æ–ø—ã—Ç–∫–∏)
        recent_performance = 0
        early_performance = 0
        
        for i in [0, 1, 2]:  # –ü–µ—Ä–≤—ã–µ 3
            early_performance += row.get(f'hist_{i}_score', 0.0)
        for i in [7, 8, 9]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3
            recent_performance += row.get(f'hist_{i}_score', 0.0)
        
        performance_trend = recent_performance - early_performance
        features.append(performance_trend)
        
        # 5. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏—è –∏ —É—Ä–æ–≤–Ω—è —Å—Ç—É–¥–µ–Ω—Ç–∞
        difficulty_match = abs(task_difficulty - (skill_learned * 2))
        features.append(difficulty_match)
        
        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        hist_scores = [row.get(f'hist_{i}_score', 0.0) for i in range(10)]
        consistency = 1.0 - np.std(hist_scores) if len(hist_scores) > 1 else 1.0
        features.append(consistency)
        
        # 6. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # ID –∑–∞–¥–∞–Ω–∏—è (–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)
        task_id_encoded = (row['task_id'] % 100) / 100.0
        features.append(task_id_encoded)
        
        features_list.append(features)
        targets.append(float(row['target']))
    
    return np.array(features_list), np.array(targets)

class EnhancedDKN(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å attention –∏ residual connections"""
    
    def __init__(self, input_size):
        super().__init__()
        
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫ —Å batch normalization
        self.input_block = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Attention –º–µ—Ö–∞–Ω–∏–∑–º (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        self.attention = nn.Sequential(
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Sigmoid()
        )
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–∏ —Å residual connections
        self.hidden1 = nn.Sequential(
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.hidden2 = nn.Sequential(
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output = nn.Sequential(
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # –í—Ö–æ–¥–Ω–æ–π –±–ª–æ–∫
        x1 = self.input_block(x)
        
        # Attention
        attention_weights = self.attention(x1)
        x1_attended = x1 * attention_weights
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–∏
        x2 = self.hidden1(x1_attended)
        x3 = self.hidden2(x2)
        
        # –í—ã—Ö–æ–¥
        output = self.output(x3)
        return output.squeeze()

def enhanced_train():
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ª—É—á—à–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    logger.info("=== –£–õ–£–ß–®–ï–ù–ù–û–ï –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï DKN ===")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_csv('mlmodels/dkn/dataset/enhanced_synthetic_dataset.csv')
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        df_sample = df.sample(n=min(50000, len(df)), random_state=42).reset_index(drop=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X, y = create_enhanced_features(df_sample)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.25, random_state=42, stratify=y_train
        )
        
        logger.info(f"–î–∞–Ω–Ω—ã–µ: {len(X_train)} train, {len(X_val)} val, {len(X_test)} test")
        
        # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_val = torch.tensor(X_val, dtype=torch.float32).to(device)
        y_val = torch.tensor(y_val, dtype=torch.float32).to(device)
        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test = torch.tensor(y_test, dtype=torch.float32).to(device)
        
        # 3. –ú–æ–¥–µ–ª—å
        model = EnhancedDKN(X_train.shape[1]).to(device)
        logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –Ω–∞ {device}")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
        
        # 4. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=0.001, 
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2, eta_min=1e-6
        )
        
        criterion = nn.BCELoss()
        
        # 5. –û–±—É—á–µ–Ω–∏–µ —Å early stopping
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        
        best_val_acc = 0.0
        patience = 5
        patience_counter = 0
        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
        
        start_training = time.time()
        
        for epoch in range(200):  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            optimizer.zero_grad()
            
            train_pred = model(X_train)
            train_loss = criterion(train_pred, y_train)
            train_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model.eval()
            with torch.no_grad():
                val_pred = model(X_val)
                val_loss = criterion(val_pred, y_val)
                
                # –ú–µ—Ç—Ä–∏–∫–∏
                train_acc = ((train_pred > 0.5) == y_train).float().mean().item()
                val_acc = ((val_pred > 0.5) == y_val).float().mean().item()
            
            epoch_time = time.time() - start_time
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            history['train_loss'].append(train_loss.item())
            history['val_loss'].append(val_loss.item())
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            if epoch % 5 == 0 or epoch < 10:
                lr = optimizer.param_groups[0]['lr']
                logger.info(f"–≠–ø–æ—Ö–∞ {epoch+1:2d} ({epoch_time:.1f}s): "
                           f"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
                           f"Val Loss={val_loss:.4f}, Acc={val_acc:.4f} | LR={lr:.6f}")
            
            # Early stopping –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                torch.save(model.state_dict(), 'best_enhanced_model.pth')
                if epoch % 5 == 0 or val_acc > 0.7:
                    logger.info(f"  >>> –ù–æ–≤–∞—è –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    break
        
        training_time = time.time() - start_training
        
        # 6. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
        model.load_state_dict(torch.load('best_enhanced_model.pth'))
        model.eval()
        
        with torch.no_grad():
            test_pred = model(X_test)
            test_loss = criterion(test_pred, y_test)
            test_acc = ((test_pred > 0.5) == y_test).float().mean().item()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            test_pred_prob = test_pred.cpu().numpy()
            test_true = y_test.cpu().numpy()
            
            # Precision, Recall, F1
            tp = np.sum((test_pred_prob > 0.5) & (test_true == 1))
            fp = np.sum((test_pred_prob > 0.5) & (test_true == 0))
            fn = np.sum((test_pred_prob <= 0.5) & (test_true == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # 7. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        logger.info(f"üìä –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
        logger.info(f"üéØ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.4f}")
        logger.info(f"üìà Precision: {precision:.4f}")
        logger.info(f"üìà Recall: {recall:.4f}")
        logger.info(f"üìà F1-score: {f1:.4f}")
        logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f}—Å")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'best_validation_accuracy': best_val_acc,
            'test_accuracy': test_acc,
            'test_precision': precision,
            'test_recall': recall,
            'test_f1': f1,
            'training_time': training_time,
            'num_epochs': len(history['train_acc']),
            'history': history,
            'model_parameters': sum(p.numel() for p in model.parameters()),
            'data_size': len(df_sample),
            'features_count': X.shape[1]
        }
        
        with open('enhanced_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ enhanced_results.json")
        
        return model, results, scaler
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None, None

if __name__ == "__main__":
    model, results, scaler = enhanced_train()
    if results:
        print(f"\nüöÄ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['test_accuracy']:.4f}")
        print(f"üìà F1-score: {results['test_f1']:.4f}")
        print(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {results['model_parameters']:,}")
        print(f"üìù –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {results['features_count']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {results['training_time']:.1f}—Å")
        print(f"üì¶ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {results['data_size']:,} –ø—Ä–∏–º–µ—Ä–æ–≤")
