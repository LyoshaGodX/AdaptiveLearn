      Интеллектуальный компонент разрабатываемой адаптивной системы представляет собой комплексную архитектуру, объединяющую агента обучения с подкреплением, модели отслеживания знаний обучающихся, гибридную экспертную систему и языковую модель. В основе лежит формализация персонализированного обучения как задачи Марковского процесса принятия решений, в которой состояние s??S характеризует познавательный профиль студента (вектор вероятностей освоения всех навыков), действие a?A – выбранное системой педагогическое воздействие (например, предъявление задания определённого типа или сложности), а функция вознаграждения r(s,a) задаётся так, чтобы стимулировать максимальное повышение уровня знаний. Цель агента – найти оптимальную стратегию ?^: S?A, максимизирующую ожидаемое дисконтированное суммарное вознаграждение E[?_t???^t r_t ?]. Для приближённого решения используется алгоритм Deep Q-Network (DQN), в котором оптимальная Q-функция Q^((s,a) ) оценивается нейронной сетью Q(s,a;?). Алгоритм DQN выполняет итеративную оптимизацию параметров ? методом стохастического градиентного спуска, минимизируя функцию потерь на основе уравнения Беллмана. В каждый итерационный шаг i решается задача минимизации ошибки временного различия [18][24]:
?(L_i ?(?_i ?)=E_((s,  a,r,s^' )?U(D) ) ?[(r+???max?_(a^' ) ??Q(s^',a^';?_i^- ?)-Q(s,a;?_i ?))^2 ],#(1) )
где выборка переходов (s,a,r,s^' ) берётся из буфера опыта D (опыта агента, накопленного в процессе взаимодействия). Здесь ?_i – параметры основной Q-сети на итерации i, а ?_i^- – параметры вспомогательной целевой сети, обновляемой с задержкой для стабилизации обучения. Внедрение механизма перемешивание переходов в буфере и фиксированной целевой сети разрывает статистические зависимости между последовательными наблюдениями и снижает корреляцию между Q(s,a) и целевым значением r+???max?_(a^' ) ??Q(s^',a^';?_i^- ?), что обеспечивает устойчивую сходимость алгоритма. В результате обучения нейронный аппроксиматор сходится к функции Q(s,a)?Q^* (s,a), и жадная политика ?(s)=arg??max?_a?Q  (s,a;?) приближается к оптимальной стратегии персонализации.
      Однако для эффективного старта обучения агента требуется удачная инициализация параметров ?, особенно в условиях ограниченного объёма данных о реальных обучающихся (проблема холодного старта). В решении этой проблемы используется метод мета-обучения Model-Agnostic Meta-Learning (MAML) [19], позволяющий реализовать few-shot learning для агента DQN. Идея MAML состоит в том, чтобы найти такие начальные параметры ?^* модели, которые обеспечивают высокое качество после небольшого числа градиентных шагов адаптации на новой задаче. Формально вводится множество задач T_i, моделирующих различные сценарии обучения (например, различные симулированные профили студентов), на каждой из которых определена функция потерь L_(T_i ) (?). В процессе мета-тренировки параметры обновляются по двухуровневой схеме. Внутренний цикл адаптирует текущие параметры ? на каждой задаче T_i с помощью одного или нескольких шагов градиентного спуска, для одного шага ??^'?_i=?-???L_(T_i)^"train"  (?). Внешний цикл оптимизирует исходные параметры ? по эффективности на всех задачах после внутренней адаптации. Градиент мета-цели складывается из вкладов каждой задачи:
?(???-???_i?(?_???L_(T_i)^val ??(?_i^' ?)) ,#(2) )
где L_(T_i)^"val"  (?^' i) – значение функции потерь на контрольном подмножестве задачи T_i после обновления до ?^' i. Таким образом, MAML оптимизирует ? так, чтобы одна-две итерации обучения на новых данных приводили к максимальному улучшению качества. В контексте DQN это означает, что начальные веса нейросети агента обучаются на множестве вспомогательных сред (например, симуляций студентов), благодаря чему агент сможет быстро адаптироваться к реальным обучающимся, получив минимальный объем взаимодействий. Формально, начальный аппроксиматор Q(s,a;?^* ) после дообучения на данных нового студента D_"new"  (через градиентные шаги по L_"new" ) будет удовлетворять ?^*=arg?min??E??T_i?p(T)?  LT_i^"val"  (?-??_? L_(T_i)^"train"  (?)). За счёт градиента через MAML выполняет двухуровневую оптимизацию, требующую вычисления производных второго порядка (градиентов по градиентам), что реализуется посредством автоматического дифференцирования (TensorFlow). Применение MAML для инициализации DQN позволяет агенту в рамках адаптивной системы обучения эффективно решать задачи тонкой настройки политики под нового обучающегося, используя лишь несколько траекторий взаимодействия, что существенно снижает влияние холодного старта. 
      Следующим ключевым компонентом архитектуры является модель Bayesian Knowledge Tracing, предназначенная для отслеживания прогресса студентов по отдельным навыкам. Модель BKT описывается скрытой марковской моделью [20], в которой бинарная переменная L_t указывает, освоил ли студент рассматриваемый навык к моменту шага t. Для каждого навыка BKT задаёт четыре параметра P(L_0 ) – априорная вероятность того, что студент владеет навыком до начала обучения; P(T) – вероятность усвоения навыка в результате одного предъявления соответствующего задания; P(G) – вероятность удачного угадывания ответа при отсутствии знания; P(S) – вероятность случайной ошибки при наличии знания. Предполагается, что P(T) не зависит от номера попытки и одинаково для всех шагов освоения навыка. Динамика изменения вероятности освоения рассчитывается по рекуррентному соотношению если P(L_(t-1) ) – вероятность овладения навыком перед очередным заданием, то после шага t она обновляется до 
?(P(L_t ?)=P(L_t-1?)+P(T)(1-P(L_t-1?)), #(3) )
отражая то, что не знающий навык студент может выучить его с вероятностью P(T) (студент, уже знающий навык, остаётся в состоянии знания с вероятностью 1, т.е. модель BKT не предполагает забывание). Для связи с наблюдаемыми ответами вводится вероятность правильного решения задания на шаге t: 
?(P(C_t ?=correct)=P(G)(1-P(L_t ?))+(1-P(S))P(L_t ?),#(4) )
то есть правильный ответ может быть дан либо за счёт угадывания при незнании навыка (с вероятностью P(G)), либо за счёт применения освоенного навыка без случайной ошибки (с вероятностью 1-P(S)). При фактическом получении ответа от студента модель обновляет апостериорную вероятность знания навыка с помощью Байесовского пересчёта. Например, если ответ правильный, то апостериорная вероятность освоения навыка до шага t равна 

?(P^* (L_(t-1)?correct)=(P(Lt-1)?[1-P(S)])/(P(L_(t-1) )?[1-P(S)]+[1-P(L_(t-1) )]?P(G) ),#(5) )? 
а после учёта возможного усвоения в ходе этой попытки вероятность знания навыка на конец шага t повышается до P(L_t )=P^(L_(t-1)?correct) + [1 - P^(L_(t-1)?correct)  ], P(T). Аналогично, при неправильном ответе вычисляется апостериорная P^* (L_(t-1)?incorrect) и затем применяется переход вероятности обучения P(T). Таким образом, BKT задаёт строгую вероятностную динамику «знание – ответ» для каждого навыка, позволяя в каждый момент времени t получать оценку P(L_t ) – вероятность того, что студент действительно освоил данный навык после наблюдаемой последовательности успехов и ошибок. В рассматриваемой системе адаптивного обучения вероятность P(L_t ) для всех навыков образует вектор скрытого состояния студента. Данный вектор знаний используется агентом DQN как часть описания состояния s, а также служит входными данными для других аналитических компонентов системы.
      Для учёта междисциплинарных зависимостей в модели знаний дополнительно строится направленный ациклический граф G=(V,E), вершинами v?V которого являются навыки (знания, компетенции), а дуга (u?v)?E означает, что навык u является прямым предпосылочным требованием (пререквизитом) для изучения навыка v. Такой граф знаний вводит систему логико-педагогических ограничений на динамику моделей BKT. В частности, порядок на графе обязывает соблюдать иерархию, если навык v освоен, то все его предшествующие навыки u (для которых существуют пути u?v в графе) с высокой вероятностью также должны быть освоены к этому времени. Таким образом, если по модели отслеживания знаний студент демонстрирует мастерство в продвинутой компетенции v, система может скорректировать оценки, повысив вероятность P(L) по базовым навыкам u (например, установив P(L_u )?1 при подтверждённом освоении v). С другой стороны, пока вероятность P(L_u ) низка, обучающая система не должна предлагать задания на навык v – это правило зашито в логику агента и экспертной системы. Формально можно ввести ограничение P(L_u )?P(L_v ) для каждой дуги (u?v), а также запрещающее условие на действия ?(s) никогда не выбирает задания по навыку v, если для некоторого u из его предпосылок P(L_u ) в состоянии s ниже заданного порога. В рамках современной модели отслеживания знаний подобные отношения пререквизитов можно внедрять как мягкие ограничения при обучении нейросетевых моделей (например, добавляя в функцию потерь штраф за нарушение порядка усвоения). Так, Chen et al. Показали [20], что включение информации о структуре предметной области в виде отношений предшествования между компетенциями улучшает точность прогноза знаний студентов по сравнению со стандартными моделями KT без таких ограничений. В данной системе адаптивного обучения граф навыков служит базой знаний для экспертных правил (см. ниже) и накладывает ограничения на действия агента и динамику модели знаний, предотвращая педагогически неосмысленные траектории.
      Наряду с вероятностно-обучающими компонентами, интеллектуальная архитектура включает экспертную систему гибридного типа, сочетающую жёстко заданные правила и данные, полученные методами машинного обучения. Экспертная система отвечает за ряд решений: детектирование особых ситуаций обучения, вычисление сложных показателей, а также участвует в формировании сигнала награды r_t для агента. Правила, заложенные экспертами-предметниками, позволяют учесть априорные педагогические знания – например, задают эвристику выбора типа подсказки или рекомендуемую длительность тренировки по навыку. Однако одних правил может быть недостаточно для тонкой персонализации, поэтому их дополняет модуль машинного обучения, способный обобщать по данным об успехах предыдущих студентов. В качестве такого модуля используется градиентный бустинг решающих деревьев – алгоритм XGBoost [21]. Данный алгоритм обучается на накопленных данных взаимодействия (первоначально – на синтетических, сгенерированных моделями, а затем на реальных) решать задачи классификации/регрессии, релевантные для адаптивного обучения. В частности, XGBoost может предсказывать вероятность того, что ответ правильный именно вследствие знания, а не удачного угадывания, либо оценивать вероятность успешного освоения навыка к определённому моменту. Градиентный бустинг строит ансамбль из M деревьев f_1,…,f_M, на каждом шаге добавляя новое дерево для минимизации дифференцируемой целевой функции. Итоговая модель представляет собой сумму предсказаний деревьев F(x)=?_(m=1)^M??f_m (x) ?, оптимизируемую с регуляризацией для контроля сложности и переобучения. В контексте нашей системы входными признаками x для XGBoost могут служить параметры состояния студента (например, текущие вероятности P(L) по всем навыкам, результаты последних попыток, временные характеристики), а выходом y – некоторая целевая переменная, полезная для принятия решений. Например, одна из задач – классификация типа ответа: XGBoost маркирует каждый полученный ответ как знание или угадывание/случайность на основе паттернов ответов данного студента. Другая возможная задача – регрессия прироста знаний, предсказание величины ?P(L_k ), на которую улучшится знание навыка k после данного шага (или через несколько шагов) обучения. Алгоритмы дерева решений хорошо подходят для таких задач, так как способны улавливать нелинейные зависимости в небольших разреженных наборах данных и дают интерпретируемые признаки важности. Кроме того, XGBoost показал высокую эффективность на множестве задач классификации и ранжирования, а благодаря регуляризации и продуманным оптимизациям (например, учитывающим разреженность данных) этот алгоритм масштабируется на большие выборки без потери качества. 
      Гибридный подход сочетания правил и ML обеспечивает баланс между интерпретируемостью и адаптивностью экспертной системы. Правила задают основу (например, не допускать освоение навыка v до u; рекомендуемое время повторения упражнения и пр.), а XGBoost вносит корректировки на основе эмпирических данных. Рассчитанные экспертом показатели используются, в том числе, при вычислении мгновенного вознаграждения r_t, которое подаётся агенту DQN. В общем случае награда на шаге формируется как функция результатов действия и изменения состояния студента r_t=R(s_t,a_t,s_(t+1) ). В нашей системе естественным выбором является определение награды через прирост знания, то есть увеличение вероятности освоения соответствующего навыка. Например, если на шаге t студент получил задание по навыку k, то можно положить 
?(r_t ?=P(?L_k  на шаге t+1)-P(L_k ? на шаге t),#(6) )
то есть разность апостериорной и априорной вероятностей знания навыка k до и после выполнения задания. Такое определение поощряет те действия, которые приводят к максимальному увеличению знаний, даже если сам ответ был неверным (ведь ошибка тоже может послужить обучающей возможностью). На практике же сигнал вознаграждения может быть более сложным, комбинируя бинарную составляющую за правильность ответа и непрерывную – за величину усвоения. Гибридная экспертная система позволяет тонко настроить эту комбинацию. Так, правила могут, к примеру, устанавливать базовое вознаграждение +1 за верный ответ и 0 за неверный, а модуль XGBoost – вносить поправки, вычитать штраф, если верный ответ классифицирован как угадывание, или добавлять бонус за неправильный ответ, который дал новую информацию (например, если модель прогнозировала высокую вероятность правильного ответа, а студент ошибся, то обнаружение пробела в знании может быть ценно для дальнейшего обучения). Формально, можно ввести классификатор h_"gs"  для детекции угадывания/случайности и определить награду как r_t=I(?correct?_t )?-????I(?correct?_t  классифицирован h_gs  как угадывание), где I(?"correct" ?_t ) – индикатор правильного ответа на шаге t, а I… – индикатор события в фигурных скобках. Коэффициент ? задаёт штраф за неинформативные верные ответы. Такое правило вознаграждения побуждает агента различать ситуации истинного усвоения от случайного успеха, опираясь на оценку классификатора, и стремиться к траекториям, где знания студента растут истинно, а не только по внешним проявлениям.
      Наконец, в архитектуру интегрирована языковая модель – компонент обработки естественного языка, позволяющий системе генерировать персонализованные текстовые подсказки, объяснения или анализировать ответы студентов в свободной форме. Ввиду ограничений вычислительных ресурсов и необходимости быстрых откликов используется подход дообучения предобученной большой языковой модели с помощью методики Low-Rank Adaptation (LoRA). LoRA позволяет адаптировать большие модели (например, трансформеры) к новой задаче, добавляя в каждый слой небольшие низкоранговые матрицы-адаптеры и обучая только их, замораживая исходные веса модели [22]. За счёт этого число обучаемых параметров сокращается на несколько порядков (для модели GPT-3 с 175 млрд параметров сокращение ~10000?) без заметной потери качества. В нашем случае языковая модель после дообучения методом LoRA специализируется на образовательном домене и стиле общения с обучающимися – она генерирует обоснование рекомендации с учётом профиля знаний студента. Важно, что добавление низкоранговых адаптеров практически не увеличивает время вывода (инференса) модели, поскольку во время выполнения они могут быть объединены с основными матрицами трансформера. Тем не менее, для гарантированного соблюдения ограничений по времени отклика и уместности генерируемых подсказок в систему заложены ограничения на максимальную длину вывода и глубину рассуждений языка модели. Так, генерация ответа принудительно завершается после определённого количества токенов, а в случае превышения порога времени (несколько сотен миллисекунд) возвращается заранее сконструированная алгоритмически шаблонная реплика. Эти меры предосторожности обеспечивают детерминированность и быстродействие языкового компонента, не позволяя ему задерживать работу системы. В целом, включение языковой модели обогащает адаптивную систему возможностями интерактивного обучения, компонент DQN выбирает, что предъявить студенту, а языковый модуль помогает решить, как именно это обосновать. Использование метода LoRA при этом минимизирует требуемый объём данных для обучения языковой модели и вписывается в ограничения холодного старта.
      Особое внимание в разработке интеллектуальной архитектуры уделяется проблеме холодного старта – отсутствию исторических данных по реальным обучающимся на момент запуска системы. В силу этого все описанные компоненты изначально настраиваются и обучаются на синтетических данных, сгенерированных с использованием априорных моделей. Сгенерированные данные имитируют логи взаимодействия студентов с системой и позволяют предварительно разогреть агентные и диагностические алгоритмы, избежав их изначальной неинформативности. Формально, генеративная модель симулированного студента строится на основе тех же вероятностных допущений, что и BKT [23]. Для каждого навыка k фиксируются параметры (P(L_0^k ),P(T^k ),P(G^k ),P(S^k )), задающие модель усвоения и ответа. Затем для генерации сценариев обучающего взаимодействия итеративно моделируется процесс, начальное скрытое состояние студента L_0^k для каждого навыка выбирается согласно P(L_0^k ) (случайно). На каждом шаге t симуляции система (агент или заданная стратегия) выбирает некоторое действие a_t, соответствующее предъявлению задания по навыку j="skill" (a_t ). Вероятность правильного ответа задаётся выражением модели BKT, P(C_t=1)=P(G^j ),[1-L_(t-1)^j ]+[1-P(S^j )],L_(t-1)^j, где L_(t-1)^j?0,1 – фактическое знание навыка j у данного симулированного студента перед шагом (скрытая переменная). Исход ответа генерируется путём пробы Бернулли с этой вероятностью. После генерирования ответа симулятивный студент обновляет своё знание, если L_(t-1)^j=0 (навык не был освоен к шагу t), то переходит в состояние L_t^j=1 (усваивает навык) с вероятностью P(T^j ); в противном случае (L_(t-1)^j=1) остаётся в L_t^j=1. Таким образом, скрытое состояние студента эволюционирует согласно тем же уравнениям, что и в модели BKT, но здесь мы рассматриваем именно истинное знание симуляции, не наблюдаемое напрямую. Для каждой такой транзакции (s_t,a_t,r_t,s_(t+1) ) система формирует запись в логе, состояние s_t включает текущий вектор вероятностей знания P(L_t ) (вначале равный заданным P(L_0 ), впоследствии обновляемый согласно BKT-динамике на основе сгенерированных ответов), действие a_t – идентификатор предъявленного задания/навыка, результат (оценка успеха) преобразуется в награду r_t, а новое состояние s_(t+1) содержит обновлённый вектор вероятностей знания. Сгенерированные таким образом последовательности (s,a,r,s^' ) имитируют траектории взаимодействия множества виртуальных студентов с адаптивной системой, включая сценарии разнообразных стратегий обучения. Эти синтетические данные используются для предварительного обучения всех компонент, агент DQN оптимизирует свою Q-функцию, обучаясь на сгенерированных эпизодах как на опыте взаимодействия; модель BKT калибруется так, чтобы параметры (P(L_0 ),P(T),P(G),P(S)) наилучшим образом объясняли смоделированные ответы (по сути, мы можем взять истинные значения параметров, использованные в симуляции, тем самым задав начальную точку); модуль XGBoost обучается классифицировать события по смоделированным признакам (распознавать угадывания на основе тех случаев, когда симулированный студент дал правильный ответ при L_(t-1)=0). Благодаря тому, что в симуляции известны истинные скрытые переменные, можно сгенерировать обучающие примеры для множества задач машинного обучения, которые в реальных данных напрямую не наблюдаются. Например, XGBoost можно обучить по синтетике предсказывать вероятность L_t^k=1 (истинного знания) по истории ответов – фактически имитируя функциональность BKT другим методом – или предсказывать вероятность L_t^k при наличии межнавыковых зависимостей (учитывая граф G навыков). Кроме того, имитационная среда позволяет сгенерировать разнообразные ситуации, которые редко встречаются в ограниченном реальном датасете, тем самым разносторонне подготовив модель агента и эксперта.
      После развёртывания системы и накопления эмпирических данных по реальным обучающимся все компоненты проходят процедуру последующего дообучения (fine-tuning) на этих данных с целью постепенной адаптации модели к настоящей аудитории. Для агента DQN это реализуется посредством пополнения буфера воспроизведения новыми переходами из взаимодействия с реальными студентами и периодического продолжения обучения нейросети на смеси старых (симуляционных) и новых примеров. Такой подход – повтор с расширяющейся базой – позволяет алгоритму обучения с подкреплением учитывать особенности поведения реальных пользователей, не забывая при этом базовые стратегии, заложенные на этапе симуляции. С течением времени веса ? Q-сети будут всё больше отражать эмпирическую динамику обучения, а политика ?(s) – подстраиваться под реальные распределения состояний. Для модели BKT процедура дообучения означает байесовский пересчёт её параметров. Имея априорные оценки (априор P(L_0 ) можно взять из оценок входного тестирования студентов, P(T) – из литературы и/или из симуляции), система по мере поступления данных корректирует их через максимизацию функции правдоподобия. Поскольку BKT обычно обучают методами максимального правдоподобия (EM-алгоритмом), можно после каждого заданного объёма новых данных запускать EM-процедуру, начиная с предыдущих параметров как стартового приближения (априора), получая обновлённые P(L_0 )"new",P(T)"new",P(G)"new",P(S)"new" . В терминах байесовского вывода можно трактовать исходные параметры как априорное распределение и выполнять апостериорный пересчёт с учётом новой информации (апостериорная бета-оценка для вероятностей, переформулируя BKT как модель с бинарными скрытыми переменными и биномиальными наблюдениями). Эта регулярная перекалибровка BKT гарантирует, что модель остаётся точной при изменении контингента обучающихся или появлении новых типов заданий. Аналогично, модуль XGBoost подлежит периодическому переобучению на накопленной выборке реальных данных. Поскольку этот алгоритм не обучается в режиме онлайн инкрементно, реализуется стратегия воздержания, раз в неделю модель переобучается на всём массиве накопленных логов взаимодействий (либо на скользящем окне недавних данных) заново, благодаря чему учитывает актуальные тенденции и не фиксирует артефакты синтетической генерации. При каждом таком переобучении может выполняться калибровка вероятностных прогнозов (посредством изотонической регрессии на отложенной выборке) для обеспечения корректной интерпретации выходов в качестве вероятностей. Гибридная экспертная система также допускает тонкую корректировку правил на основе данных, если обнаруживается систематическое расхождение между решениями ML-модуля и заложенными правилами, эксперт может обновить правило или ввести новое, а ML-модель – учесть это при следующем обучении.


      Разработанная адаптивная обучающая система включает пять основных компонентов, взаимосвязанных между собой. Агент на основе Deep Q-Network (DQN) для рекомендации заданий, модель отслеживания знаний Bayesian Knowledge Tracing (BKT) для оценки уровня освоения навыков, модуль быстрого обучения на малых выборках (Few-Shot Learning) с мета-алгоритмом MAML для оперативной адаптации под нового студента, дообученную методом LoRA языковую модель (LLM) для генерации объяснений рекомендаций, и гибридную экспертную систему на базе правил и градиентного бустинга XGBoost для автоматической оценки выполнения заданий. Все компоненты реализованы в рамках единой программной архитектуры на языке Python; для нейросетевых моделей используется фреймворк PyTorch (в частях агентной и мета-обучения), для байесовских моделей – платформа вероятностного программирования PyMC3, для методов обработки языка – библиотека HuggingFace Transformers, а для экспертых правил – движок продукционных правил (CLIPS) с интеграцией через PyCLIPS. В качестве оркестрации компонент выбрана микросервисная схема с обменом данными через gRPC, адаптивный модуль рекомендаций развернут как отдельный сервис, предоставляющий методы Recommend(state) для запроса следующего задания и Feedback(result) для получения результата и обновления моделей, что обеспечивает слабую связанность компонентов и масштабируемость системы.
      За рекомендацию оптимальных учебных заданий отвечает агент с глубоким Q-обучением. Он формирует политику выбора заданий, максимизирующую кумулятивное вознаграждение – прогресс студента в освоении навыков – рассматривая адаптацию траектории обучения как задачу управления с подкреплением. Состояние среды при этом определяется текущим знанием студента (вектор вероятностей освоения всех навыков по BKT) и историческими данными о прохождении задания; действие – выбор следующего задания из доступного банка. Нейронная сеть агента реализована как сверточная нейросеть с двумя полносвязными слоями, на выходе которой генерируются Q-значения для каждого возможного задания. Такой архитектурный выбор продиктован успешным опытом применения DQN для задач с большим пространством состояний [30]. Реализация выполнена в PyTorch; сеть обучается офлайн на синтетических данных (см. ниже) и дообучается онлайн по мере накопления данных взаимодействия со студентами. Для стабильности обучения применяются целевая сеть и воспроизведение опыта. Агент DQN инкапсулирован в отдельный сервис, принимающий на вход текущее состояние студента (сериальный вектор признаков знаний) и возвращающий идентификатор рекомендованного задания. Обмен данными между фронтендом/бэкендом и сервисом агента происходит в формате Protocol Buffers по gRPC, что минимизирует задержки и обеспечивает языковую агностику интерфейсов. Выбор технологии RL-агента обусловлен способностью методов с подкреплением оптимизировать долгосрочный образовательный эффект последовательности заданий, превосходя жадные стратегии.
      Для представления знания студента о каждом навыке используется байесовская модель Knowledge Tracing. В ее основе – скрытая модель Маркова, где каждый навык считается либо «усвоенным», либо «неусвоенным» (бинарное скрытое состояние), а обучение моделируется как переход из состояния незнания в знание при успешном выполнении соответствующих заданий. В данной системе реализован классический вариант BKT: для каждого навыка хранятся вероятности P(L?) первоначального знания, P(T) усвоения при получении правильного ответа, P(G) угадывания (вероятность правильного ответа при незнании) и P(S) ошибки (вероятность неправильного ответа при знании). Обновление вероятности знания навыка после каждого ответа вычисляется по формулам байесовского вывода с учетом параметров guessing/slip. Модель BKT реализована с помощью PyMC3 [26], что позволяет явно задавать априорные распределения этих параметров и обновлять апостериорные оценки по данным студентов. После каждого ответа студента система передает модулю BKT информацию о том, какое задание (и по какому навыку) выполнено и с каким результатом; на выходе BKT выдает обновленную апостериорную вероятность мастерства данного навыка. Эти вероятности хранятся как в оперативной памяти (для использования агентом DQN при рекомендации следующего шага обучения), так и в базе данных состояния студентов для долговременного хранения. Формат представления состояния – вектор размерности N (число навыков) с компонентами, равными текущим вероятностям освоения; в реализации хранение организовано в виде сериализованного JSON-объекта, сопоставленного каждому студенту и таблицы БД с колонками (student_id, skill_id, mastery_prob). Выбор BKT объясняется широкой распространенностью этого подхода для точного поэтапного обновления модели знаний на основе последовательности ответов, а также его интерпретируемостью – вероятности по BKT легко объяснимы педагогически (низкая вероятность по навыку указывает на необходимость повторения материала).
Чтобы система могла быстро подстроиться под нового студента с минимальным количеством начальных заданий, внедрен механизм мета-обучения на основе алгоритма Model-Agnostic Meta-Learning (MAML). Идея состоит в том, что параметры моделей адаптивного модуля (в частности, параметров DQN-агента либо начальных знаний BKT) предварительно тренируются на множестве задач таким образом, чтобы требовалось лишь несколько шагов градиентного спуска по небольшому объему новых данных для их настройки под конкретного студента [27]. Реализация MAML-модуля выполнена в PyTorch, вовремя мета-тренировки имитируются разные задачи – профили студентов и их траектории обучения – на которых агент и модели обучаются таким образом, чтобы минимизировать ошибку после последующего небольшого обновления параметров. В частности, для MAML выбрано 3 шага градиентного спуска с малым шагом обучения (порядка 1e-4) для адаптации, что, согласно литературе, обеспечивает баланс между скоростью перенастройки и стабильностью. В рабочем режиме, при появлении нового студента, система проводит краткую калибровку, результаты его входного тестирования используются для выполнения нескольких итераций дообучения модели с помощью заранее подготовленного мета-алгоритма. Это приводит к тому, что система подстраивается под индивидуальные особенности студента буквально после нескольких взаимодействий, что повышает качество персонализации с самого начала обучения. Использование MAML обосновано тем, что мета-обучение позволяет существенно ускорить индивидуализацию моделей, обучая их быть готовыми к быстрому обучению на новых данных, что критически важно для образовательных систем, где длительный период проб и ошибок с реальным студентом неприемлем.
      Компонент генерации объяснений реализован на основе большой языковой модели трансформерной архитектуры (порядка 7 млрд параметров), адаптированной под задачи системы методом Low-Rank Adaptation (LoRA). LoRA-файнтюнинг выполнен с использованием библиотеки HuggingFace Transformers и фреймворка PEFT, исходная предобученная LLM общего назначения (Qwen2-7B) была расширена введением дополнительных низкоранговых матриц весов, обучаемых на целевой задаче, тогда как основные параметры модели зафиксированы. Такой подход радикально снизил количество обучаемых параметров (на несколько порядков по сравнению с полной донастройкой всей модели, 7.62B параметров) и позволил эффективно обучить модель генерировать краткие педагогические объяснения без деградации основных языковых способностей. Для обучения этой компоненты был сформирован специальный датасет пар «состояние студента/рекомендуемое действие – объяснение», частично на основе экспертных правил и синтетически сгенерированных шаблонов. Модель научилась выдавать сжатые (до 200 символов) обоснования выбранного задания, например, «Задание по теме X рекомендовано, поскольку предыдущие попытки по навыку X были неуспешны, и вероятность освоения навыка составляет лишь ~40%». Генерация объяснения запускается сразу после выбора агентом задания, система формирует текстовый prompt, включающий информацию о навыках студента и рекомендуемом упражнении, и передает его в LLM, получая на выходе готовый пояснительный текст. Для ускорения вывода реализована оптимизация инференса – модель работает в половинной точности на GPU, добиваясь времени ответа менее 30 мс за счет ограничения длины выхода и использования сжатых LoRA-представлений. Компонент LLM задействуется по необходимости, что повышает прозрачность работы системы. Применение методики LoRA при дообучении модели обусловлено тем, что полная тонкая настройка больших моделей требует огромных ресурсов, тогда как добавление небольшого числа обучаемых параметров в каждую слой трансформера сохраняет качество модели, значительно снижая требования к вычислительной памяти.
      Для автоматической проверки и оценивания выполненных студентом заданий внедрена экспертная подсистема, объединяющая правила и алгоритмы машинного обучения. Правила, заданные экспертом предметно, реализованы на продукционной системе (CLIPS) и покрывают формальные критерии оценки, например, для заданий с выбором ответа – соответствие выбранного варианта верному, для задач по математике – соответствие полученного решения известному образцу (с допуском эквивалентных алгебраических преобразований), для программирования – прохождение всех тестовых случаев на сервере. Правила оформлены в виде набора продукции (IF-THEN) в базе знаний системы и исполняются движком forward-chaining; при срабатывании правил формируется предварительный вывод о результате (правильно/неправильно, обнаруженные ошибки). Однако не все аспекты ответов могут быть строго закодированы правилами. Поэтому в случае комплексной оценки открытого ответа привлекается модель XGBoost – градиентный бустинг деревьев решений, обученный на накопленной базе предыдущих ответов. Модель XGBoost использует разнообразные признаки ответа (время решения, число попыток, отклонение от оптимального решения, текстовые характеристики ответа и др.) и возвращает вероятностную оценку корректности, которые затем интерпретируются системой. XGBoost был выбран ввиду его высокой точности и производительности на задачах классификации и регрессии в образовательных данных [29], а также удобства интерпретации важности признаков. Обучение XGBoost-модели произведено на синтетически сгенерированном датасете решений (см. ниже) с добавлением ручной разметки от экспертов для ряда сложных случаев. Гибридный подход объединяет преимущества прозрачных экспертных правил и статистической алгоритмической точности ML-модели. Результат работы экспертной системы оформляется в виде структуры TaskResult, включающей уникальный идентификатор задания, идентификатор студента, бинарный флаг или балл за решение, время, затраченное студентом, и метаинформацию (тип допущенной ошибки, уровень сложности и т.д.). Этот объект сериализуется в JSON и сохраняется в базе данных ответов, а также передается модулю адаптации (агенту DQN и BKT) для последующего обновления знаний студента и расчета вознаграждения RL-агента. Таким образом, обратная связь замыкается, результаты, автоматически выставленные экспертной системой, используются для обучения агента рекомендаций и корректировки модели знаний.
      Система использует реляционную базу данных (PostgreSQL) для хранения основной информации о предметной области и пользователях. В базе данных определены сущности Курс, Навык, Задание, Студент и др., со связями между ними, навыки привязаны к курсам (многие-ко-многим), задания относятся к определенным навыкам и хранят такие атрибуты, как текст условия, уровень сложности, правильный ответ/решение, и пр. Граф зависимости навыков хранится явно через таблицу связей «предпосылка–навык», что позволяет извлекать все прямые и транзитивные зависимости для любого навыка. База знаний курсов и навыков, таким образом, представляет семантический граф, необходимый для генерации траекторий обучения. Для хранения результатов ответов студентов и динамики их знаний предусмотрено два уровня хранилища. Оперативные данные (текущие состояния mastery по навыкам, последние результаты) хранятся в PostgreSQL в таблицах StudentSkillMastery и Responses. Каждая запись ответа содержит ссылку на студента, на задание, отметку о корректности (или полученный балл), временную метку, а также поле с метаданными (флагом «была подсказка», числом попыток и т.д.). Это позволяет делать быстрые выборки для аналитики успеваемости и отображения результатов. Для долговременного хранения подробной истории взаимодействия и удобства аналитической обработки больших объемов данных задействуется NoSQL-репозиторий (MongoDB), где для каждого студента накапливается документ-журнал со всеми попытками, включающий детали контекста. Поэлементные попытки, вероятности по BKT до и после каждого ответа, действия агента, сгенерированные объяснения LLM. Хранение истории в JSON-формате дает гибкость в расширении схемы и облегчает выгрузку данных для офлайн-анализа. Дополнительно развернуто аналитическое хранилище на колоночной БД (ClickHouse) для агрегации больших массивов ответов и быстрого выполнения сложных запросов (для дашбордов прогресса по группе студентов). Таким образом, инфраструктура данных комбинирует достоинства SQL и NoSQL, строгую схему для ключевых сущностей и гибкое хранение для детализированных логов. Взаимодействие между компонентами системы тщательно спроектировано, веб-приложение (Django) обслуживает HTTP-запросы пользователя и через внутренний API вызывает соответствующие сервисы – агент рекомендаций (по gRPC) для получения следующего задания, экспертную систему (по RPC) для оценки присланного решения, модуль LLM (через REST API эндпоинт генерации текста) для получения объяснения и т.д. Обмен данными стандартизован – используется JSON-сериализация и protobuf-сообщения, в которых состояния студентов, результаты и прочие объекты имеют строго заданные схемы. Например, состояние студента передается как словарь {skill_id: mastery_prob}, рекомендация возвращается как JSON {"task_id": X, "explanation": "…"}, а результат оценки – как JSON {"score": Y, "error_type": Z, "time": T, ...}. 
      Поскольку для первоначального обучения описанных моделей требуются значительные объемы данных взаимодействия студентов с системой, которые на этапе разработки отсутствовали, был разработан механизм генерации реалистичных синтетических датасетов. Синтетические данные служат для инициализации и предобучения компонентов системы, агента DQN, модели BKT, классификатора XGBoost и LLM-интерпретатора. Генерация начинается с моделирования профилей виртуальных студентов. Для каждого синтетического студента случайно задается вектор начальных уровней знаний по всем навыкам (из распределения Бета со средним значением вокруг 0.3–0.5, что имитирует частичное владение темами перед обучением). Кроме того, индивидуализируются параметры поведения, каждому студенту назначаются свои вероятность угадывания p(G) и склонность к случайным ошибкам p(S), выбранные из разумных диапазонов (p(G) ~ 0.2, p(S) ~ 0.1 в среднем, с разбросом ±0.05 между студентами) — это значит, что одни студенты могут чаще угадывать ответы наудачу, а другие, даже зная материал, иногда ошибаются по невнимательности. Также задаются параметры обучаемости, вероятность выучить новый навык при первой же успешной попытке (P(T)) и ее изменение при повторных попытках (имитация эффекта затвердевания знаний при повторении). На основе таких профилей генерируются траектории обучения, симулируется процесс, в котором виртуальный студент последовательно получает задания и «решает» их. Последовательности заданий можно генерировать случайно или по упрощенной стратегии (например, чередуя задания по разным навыкам или фокусируясь на слабых навыках), чтобы имитировать как разнообразные, так и целенаправленные сценарии обучения. На каждом шаге симуляции исход определяется текущим состоянием студента и элементом случайности, для задания, связанного с определенным навыком, вероятность правильного ответа берется равной текущей вероятности мастерства навыка (по внутренней «истинной» модели студента) плюс случайная составляющая, учитывающая угадывание и ошибку. Например, если студент еще не усвоил навык (истинная вероятность 0.4), то модель может все же вернуть правильный ответ с вероятностью ~0.4 + фактор угадывания; если же навык освоен (~0.8), есть небольшой шанс ~0.8 – фактор ошибки, что студент ответит неверно. После генерации ответа виртуального студента его внутреннее состояние обновляется: если ответ правильный и навык был не освоен, с заданной вероятностью P(T) (0.3–0.5) состояние «усвоено» переключается в истину (имитируя процесс обучения через успешное решение); при неверном ответе состояние может оставаться прежним или, в продвинутой модели, слегка уменьшаться вероятность знания (учитывая возможность «забывания» и демотивации). Синтетические журналы попыток генерируются для тысячи виртуальных студентов по каждому из заданий, образуя корпус данных с отметками о всех взаимодействиях. Эти данные затем используются следующим образом. Для обучения агента DQN генерированный лог трактуется как опыт взаимодействия «учитель-студент», на основе которого можно провести обучение вне политик для агента рекомендаций. Пары (состояние – выбранное задание – результат – новое состояние) составляют переходы, по которым считается вознаграждение (+1 за успешное освоение навыка или за правильный ответ) и агент обновляет свои Q-оценки. По сути, агент тренируется в симуляции, чтобы впоследствии стартовать с разумной стратегией, схожей с оптимальными практиками (больше тренировать слабые навыки, чередовать повторения). Для инициализации BKT сгенерированные данные позволяют оценить априорные параметры модели знаний. С помощью MCMC-методов PyMC3 на этих данных были получены оценки P(L?), P(T), P(G), P(S) для каждого навыка, которые затем зафиксированы в модели при запуске системы с реальными студентами. Таким образом, BKT «настроена» на параметры, характерные для типичного учащегося, что предотвращает необходимость ручного задания неизвестных вероятностей. Для обучения XGBoost синтетические ответы помечены известным исходом (правильно/неправильно) и сопровождаются рассчитанными метриками (время решения, разница с оптимальным решением и др.). На этом основании выполнено обучение модели XGBoost, прогнозирующей вероятность правильного решения по совокупности признаков попытки. Полученная модель успешно воспроизводит закономерности, заложенные в генераторе (большая задержка или вторичная попытка коррелируют с неправильным ответом), и даже выявляет более сложные зависимости, что обеспечивает ей хороший старт до появления реальных данных. Для обучения LLM (LoRA) генерированные симуляции также использованы для формирования корпуса пар состояние-действие/объяснение. На основе логов автоматически составлены шаблонные объяснения, «Студент долго не может освоить навык A, поэтому далее предлагается задание по навыку A для закрепления». Эти пары послужили тренировочным набором для дообучения языковой модели, благодаря чему LLM научилась связывать состояние знаний со стратегией рекомендаций и формулировать грамотные обоснования. Несмотря на искусственный характер, синтетические данные старались делать максимально реалистичными – распределения успехов, ошибок, времен решения калибровались так, чтобы статистика соответствовала наблюдаемой в реальных учебных процессах (по данным литературы и экспертов). Такой подход согласуется с современными исследованиями, показывающими, что предварительное обучение стратегий на симулированных «виртуальных» студентах позволяет существенно сократить необходимое число взаимодействий с реальными учащимися [25]. В итоге синтетический датасет стал основой для начальной настройки всех компонентов системы, обеспечив их слаженную работу как единого адаптивного комплекса еще до привлечения первых реальных пользователей.

